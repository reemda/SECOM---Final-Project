{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASCI 420 CB\n",
    "## Milestone 3- Reem Coomes\n",
    "#### Secom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report \n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import preprocessing\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Section one include the following steps:  \n",
    "- Read both datasets Secom and Secom labels from UNI the machine learning repository \n",
    "- Concatenate both dataset into one dataframe df  \n",
    "- Change columns headers to strings \n",
    "- Examine df (head, shape, info, stats, and types) \n",
    "- Drop constant columns and the one that have missing data more than 50% \n",
    "- Fill na places with the median \n",
    "- check out the distribution of the target variable- histogram. \n",
    "- Get dummies for the target variable to transfer to 1 and 0    \n",
    "- Normalize the dataset \n",
    "- Set the date column as an index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets upload\n",
    "url1 ='https://archive.ics.uci.edu/ml/machine-learning-databases/secom/secom.data'\n",
    "url2='https://archive.ics.uci.edu/ml/machine-learning-databases/secom/secom_labels.data'\n",
    "\n",
    "# Read csv\n",
    "df1 = pd.read_csv(url1,sep='\\s+',header=None)\n",
    "df2 = pd.read_csv(url2,sep='\\s+',header=None)\n",
    "\n",
    "# Merge datasets\n",
    "df= pd.concat([df1, df2], axis=1, ignore_index=True)\n",
    "\n",
    "# Columns header as a string \n",
    "df.columns = df.columns.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m dataset: \u001b[0m \n",
      "          0        1          2          3       4      5         6       7  \\\n",
      "0  3030.93  2564.00  2187.7333  1411.1265  1.3602  100.0   97.6133  0.1242   \n",
      "1  3095.78  2465.14  2230.4222  1463.6606  0.8294  100.0  102.3433  0.1247   \n",
      "2  2932.61  2559.94  2186.4111  1698.0172  1.5102  100.0   95.4878  0.1241   \n",
      "3  2988.72  2479.90  2199.0333   909.7926  1.3204  100.0  104.2367  0.1217   \n",
      "4  3032.24  2502.87  2233.3667  1326.5200  1.5334  100.0  100.3967  0.1235   \n",
      "5  2946.25  2432.84  2233.3667  1326.5200  1.5334  100.0  100.3967  0.1235   \n",
      "6  3030.27  2430.12  2230.4222  1463.6606  0.8294  100.0  102.3433  0.1247   \n",
      "7  3058.88  2690.15  2248.9000  1004.4692  0.7884  100.0  106.2400  0.1185   \n",
      "8  2967.68  2600.47  2248.9000  1004.4692  0.7884  100.0  106.2400  0.1185   \n",
      "9  3016.11  2428.37  2248.9000  1004.4692  0.7884  100.0  106.2400  0.1185   \n",
      "\n",
      "        8       9  ...     582     583     584      585     586     587  \\\n",
      "0  1.5005  0.0162  ...  0.5005  0.0118  0.0035   2.3630     NaN     NaN   \n",
      "1  1.4966 -0.0005  ...  0.5019  0.0223  0.0055   4.4447  0.0096  0.0201   \n",
      "2  1.4436  0.0041  ...  0.4958  0.0157  0.0039   3.1745  0.0584  0.0484   \n",
      "3  1.4882 -0.0124  ...  0.4990  0.0103  0.0025   2.0544  0.0202  0.0149   \n",
      "4  1.5031 -0.0031  ...  0.4800  0.4766  0.1045  99.3032  0.0202  0.0149   \n",
      "5  1.5287  0.0167  ...  0.4949  0.0189  0.0044   3.8276  0.0342  0.0151   \n",
      "6  1.5816 -0.0270  ...  0.5010  0.0143  0.0042   2.8515  0.0342  0.0151   \n",
      "7  1.5153  0.0157  ...  0.4984  0.0106  0.0034   2.1261  0.0204  0.0194   \n",
      "8  1.5358  0.0111  ...  0.4993  0.0172  0.0046   3.4456  0.0111  0.0124   \n",
      "9  1.5381  0.0159  ...  0.4967  0.0152  0.0038   3.0687  0.0212  0.0191   \n",
      "\n",
      "      588       589  590                  591  \n",
      "0     NaN       NaN   -1  19/07/2008 11:55:00  \n",
      "1  0.0060  208.2045   -1  19/07/2008 12:32:00  \n",
      "2  0.0148   82.8602    1  19/07/2008 13:17:00  \n",
      "3  0.0044   73.8432   -1  19/07/2008 14:43:00  \n",
      "4  0.0044   73.8432   -1  19/07/2008 15:22:00  \n",
      "5  0.0052   44.0077   -1  19/07/2008 17:53:00  \n",
      "6  0.0052   44.0077   -1  19/07/2008 19:44:00  \n",
      "7  0.0063   95.0310   -1  19/07/2008 19:45:00  \n",
      "8  0.0045  111.6525   -1  19/07/2008 20:24:00  \n",
      "9  0.0073   90.2294   -1  19/07/2008 21:35:00  \n",
      "\n",
      "[10 rows x 592 columns]\n",
      "-------------------------------------------------------------------------------\n",
      "\u001b[1m dataset shape:  \u001b[0m (1567, 592)\n",
      "-------------------------------------------------------------------------------\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1567 entries, 0 to 1566\n",
      "Columns: 592 entries, 0 to 591\n",
      "dtypes: float64(590), int64(1), object(1)\n",
      "memory usage: 7.1+ MB\n",
      "\u001b[1m dataset info:  \u001b[0m \n",
      " None\n",
      "-------------------------------------------------------------------------------\n",
      "\u001b[1m dataset stats:  \u001b[0m \n",
      "                  0            1            2            3            4  \\\n",
      "count  1561.000000  1560.000000  1553.000000  1553.000000  1553.000000   \n",
      "mean   3014.452896  2495.850231  2200.547318  1396.376627     4.197013   \n",
      "std      73.621787    80.407705    29.513152   441.691640    56.355540   \n",
      "min    2743.240000  2158.750000  2060.660000     0.000000     0.681500   \n",
      "25%    2966.260000  2452.247500  2181.044400  1081.875800     1.017700   \n",
      "50%    3011.490000  2499.405000  2201.066700  1285.214400     1.316800   \n",
      "75%    3056.650000  2538.822500  2218.055500  1591.223500     1.525700   \n",
      "max    3356.350000  2846.440000  2315.266700  3715.041700  1114.536600   \n",
      "\n",
      "            5            6            7            8            9  ...  \\\n",
      "count  1553.0  1553.000000  1558.000000  1565.000000  1565.000000  ...   \n",
      "mean    100.0   101.112908     0.121822     1.462862    -0.000841  ...   \n",
      "std       0.0     6.237214     0.008961     0.073897     0.015116  ...   \n",
      "min     100.0    82.131100     0.000000     1.191000    -0.053400  ...   \n",
      "25%     100.0    97.920000     0.121100     1.411200    -0.010800  ...   \n",
      "50%     100.0   101.512200     0.122400     1.461600    -0.001300  ...   \n",
      "75%     100.0   104.586700     0.123800     1.516900     0.008400  ...   \n",
      "max     100.0   129.252200     0.128600     1.656400     0.074900  ...   \n",
      "\n",
      "              581          582          583          584          585  \\\n",
      "count  618.000000  1566.000000  1566.000000  1566.000000  1566.000000   \n",
      "mean    97.934373     0.500096     0.015318     0.003847     3.067826   \n",
      "std     87.520966     0.003404     0.017180     0.003720     3.578033   \n",
      "min      0.000000     0.477800     0.006000     0.001700     1.197500   \n",
      "25%     46.184900     0.497900     0.011600     0.003100     2.306500   \n",
      "50%     72.288900     0.500200     0.013800     0.003600     2.757650   \n",
      "75%    116.539150     0.502375     0.016500     0.004100     3.295175   \n",
      "max    737.304800     0.509800     0.476600     0.104500    99.303200   \n",
      "\n",
      "               586          587          588          589          590  \n",
      "count  1566.000000  1566.000000  1566.000000  1566.000000  1567.000000  \n",
      "mean      0.021458     0.016475     0.005283    99.670066    -0.867262  \n",
      "std       0.012358     0.008808     0.002867    93.891919     0.498010  \n",
      "min      -0.016900     0.003200     0.001000     0.000000    -1.000000  \n",
      "25%       0.013425     0.010600     0.003300    44.368600    -1.000000  \n",
      "50%       0.020500     0.014800     0.004600    71.900500    -1.000000  \n",
      "75%       0.027600     0.020300     0.006400   114.749700    -1.000000  \n",
      "max       0.102800     0.079900     0.028600   737.304800     1.000000  \n",
      "\n",
      "[8 rows x 591 columns]\n",
      "-------------------------------------------------------------------------------\n",
      "\u001b[1m dataset types:  \u001b[0m \n",
      " 0      float64\n",
      "1      float64\n",
      "2      float64\n",
      "3      float64\n",
      "4      float64\n",
      "        ...   \n",
      "587    float64\n",
      "588    float64\n",
      "589    float64\n",
      "590      int64\n",
      "591     object\n",
      "Length: 592, dtype: object\n",
      "-------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Head\n",
    "print('\\033[1m','dataset:','\\033[0m','\\n',df.head(10))\n",
    "print('-------------------------------------------------------------------------------')\n",
    "\n",
    "# Shape\n",
    "print('\\033[1m','dataset shape: ','\\033[0m',df.shape)\n",
    "print('-------------------------------------------------------------------------------')\n",
    "\n",
    "# Info\n",
    "print('\\033[1m','dataset info: ' ,'\\033[0m','\\n',df.info())\n",
    "print('-------------------------------------------------------------------------------')\n",
    "\n",
    "# Stats\n",
    "print('\\033[1m','dataset stats: ' ,'\\033[0m','\\n',df.describe())\n",
    "print('-------------------------------------------------------------------------------')\n",
    "\n",
    "# Types\n",
    "print('\\033[1m','dataset types: ' ,'\\033[0m','\\n',df.dtypes)\n",
    "print('-------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1567, 564)\n"
     ]
    }
   ],
   "source": [
    "# percentage missing in a dataframe\n",
    "percentage_missing= df.isna().mean().round(4)*100\n",
    "\n",
    "# Let's create a dataframe that hold the percentages\n",
    "missing_value_df = pd.DataFrame({'column_name': df.columns,\n",
    "                                 'percent_missing': percentage_missing})\n",
    "\n",
    "# Drop columns with more than 50% missing data\n",
    "above50=missing_value_df.loc[:,'percent_missing']>50\n",
    "x=missing_value_df.loc[above50,:]\n",
    "df.drop(x['column_name'],axis=1,inplace=True)\n",
    "print(df.shape)\n",
    "\n",
    "# Fill na values with the median\n",
    "df.fillna(df.median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1567, 299)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop constant columns \n",
    "secom_df=df.copy()\n",
    "secom_describe=secom_df.describe()\n",
    "x=secom_describe.loc[:,secom_describe.loc['std',:]<=0.1]\n",
    "x.columns\n",
    "secom_df.drop(x.columns,axis=1,inplace=True)\n",
    "secom_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Histogram for the target variable\n",
    "plt.hist(secom_df['590'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>6</th>\n",
       "      <th>12</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>...</th>\n",
       "      <th>571</th>\n",
       "      <th>572</th>\n",
       "      <th>573</th>\n",
       "      <th>574</th>\n",
       "      <th>576</th>\n",
       "      <th>577</th>\n",
       "      <th>585</th>\n",
       "      <th>589</th>\n",
       "      <th>591</th>\n",
       "      <th>590_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3030.93</td>\n",
       "      <td>2564.00</td>\n",
       "      <td>2187.7333</td>\n",
       "      <td>1411.1265</td>\n",
       "      <td>1.3602</td>\n",
       "      <td>97.6133</td>\n",
       "      <td>202.4396</td>\n",
       "      <td>7.9558</td>\n",
       "      <td>414.8710</td>\n",
       "      <td>10.0433</td>\n",
       "      <td>...</td>\n",
       "      <td>2.1113</td>\n",
       "      <td>8.95</td>\n",
       "      <td>0.3157</td>\n",
       "      <td>3.0624</td>\n",
       "      <td>1.6765</td>\n",
       "      <td>14.9509</td>\n",
       "      <td>2.3630</td>\n",
       "      <td>71.9005</td>\n",
       "      <td>19/07/2008 11:55:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3095.78</td>\n",
       "      <td>2465.14</td>\n",
       "      <td>2230.4222</td>\n",
       "      <td>1463.6606</td>\n",
       "      <td>0.8294</td>\n",
       "      <td>102.3433</td>\n",
       "      <td>200.5470</td>\n",
       "      <td>10.1548</td>\n",
       "      <td>414.7347</td>\n",
       "      <td>9.2599</td>\n",
       "      <td>...</td>\n",
       "      <td>2.4335</td>\n",
       "      <td>5.92</td>\n",
       "      <td>0.2653</td>\n",
       "      <td>2.0111</td>\n",
       "      <td>1.1065</td>\n",
       "      <td>10.9003</td>\n",
       "      <td>4.4447</td>\n",
       "      <td>208.2045</td>\n",
       "      <td>19/07/2008 12:32:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2932.61</td>\n",
       "      <td>2559.94</td>\n",
       "      <td>2186.4111</td>\n",
       "      <td>1698.0172</td>\n",
       "      <td>1.5102</td>\n",
       "      <td>95.4878</td>\n",
       "      <td>202.0179</td>\n",
       "      <td>9.5157</td>\n",
       "      <td>416.7075</td>\n",
       "      <td>9.3144</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0293</td>\n",
       "      <td>11.21</td>\n",
       "      <td>0.1882</td>\n",
       "      <td>4.0923</td>\n",
       "      <td>2.0952</td>\n",
       "      <td>9.2721</td>\n",
       "      <td>3.1745</td>\n",
       "      <td>82.8602</td>\n",
       "      <td>19/07/2008 13:17:00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2988.72</td>\n",
       "      <td>2479.90</td>\n",
       "      <td>2199.0333</td>\n",
       "      <td>909.7926</td>\n",
       "      <td>1.3204</td>\n",
       "      <td>104.2367</td>\n",
       "      <td>201.8482</td>\n",
       "      <td>9.6052</td>\n",
       "      <td>422.2894</td>\n",
       "      <td>9.6924</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0253</td>\n",
       "      <td>9.33</td>\n",
       "      <td>0.1738</td>\n",
       "      <td>2.8971</td>\n",
       "      <td>1.7585</td>\n",
       "      <td>8.5831</td>\n",
       "      <td>2.0544</td>\n",
       "      <td>73.8432</td>\n",
       "      <td>19/07/2008 14:43:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3032.24</td>\n",
       "      <td>2502.87</td>\n",
       "      <td>2233.3667</td>\n",
       "      <td>1326.5200</td>\n",
       "      <td>1.5334</td>\n",
       "      <td>100.3967</td>\n",
       "      <td>201.9424</td>\n",
       "      <td>10.5661</td>\n",
       "      <td>420.5925</td>\n",
       "      <td>10.3387</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0275</td>\n",
       "      <td>8.83</td>\n",
       "      <td>0.2224</td>\n",
       "      <td>3.1776</td>\n",
       "      <td>1.6597</td>\n",
       "      <td>10.9698</td>\n",
       "      <td>99.3032</td>\n",
       "      <td>73.8432</td>\n",
       "      <td>19/07/2008 15:22:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 299 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0        1          2          3       4         6        12  \\\n",
       "0  3030.93  2564.00  2187.7333  1411.1265  1.3602   97.6133  202.4396   \n",
       "1  3095.78  2465.14  2230.4222  1463.6606  0.8294  102.3433  200.5470   \n",
       "2  2932.61  2559.94  2186.4111  1698.0172  1.5102   95.4878  202.0179   \n",
       "3  2988.72  2479.90  2199.0333   909.7926  1.3204  104.2367  201.8482   \n",
       "4  3032.24  2502.87  2233.3667  1326.5200  1.5334  100.3967  201.9424   \n",
       "\n",
       "        14        15       16  ...     571    572     573     574     576  \\\n",
       "0   7.9558  414.8710  10.0433  ...  2.1113   8.95  0.3157  3.0624  1.6765   \n",
       "1  10.1548  414.7347   9.2599  ...  2.4335   5.92  0.2653  2.0111  1.1065   \n",
       "2   9.5157  416.7075   9.3144  ...  2.0293  11.21  0.1882  4.0923  2.0952   \n",
       "3   9.6052  422.2894   9.6924  ...  2.0253   9.33  0.1738  2.8971  1.7585   \n",
       "4  10.5661  420.5925  10.3387  ...  2.0275   8.83  0.2224  3.1776  1.6597   \n",
       "\n",
       "       577      585       589                  591  590_1  \n",
       "0  14.9509   2.3630   71.9005  19/07/2008 11:55:00      0  \n",
       "1  10.9003   4.4447  208.2045  19/07/2008 12:32:00      0  \n",
       "2   9.2721   3.1745   82.8602  19/07/2008 13:17:00      1  \n",
       "3   8.5831   2.0544   73.8432  19/07/2008 14:43:00      0  \n",
       "4  10.9698  99.3032   73.8432  19/07/2008 15:22:00      0  \n",
       "\n",
       "[5 rows x 299 columns]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -1 becomes 1 (good) - 1 becomes 0 (bad)\n",
    "secom_df=pd.get_dummies(secom_df,columns=['590'],drop_first=True)\n",
    "secom_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1567, 299)\n",
      "          0         1         2         3         4         6        12  \\\n",
      "0  0.224463  0.849523 -0.436430  0.035804 -0.050121 -0.564354  0.763117   \n",
      "1  1.107287 -0.383106  1.016977  0.155282 -0.059585  0.197639  0.181528   \n",
      "2 -1.114000  0.798901 -0.481447  0.688278 -0.047447 -0.906768  0.633530   \n",
      "3 -0.350156 -0.199072 -0.051705 -1.104376 -0.050831  0.502662  0.581382   \n",
      "4  0.242296  0.087328  1.117227 -0.156616 -0.047033 -0.115954  0.610329   \n",
      "\n",
      "         14        15        16  ...       571       572       573       574  \\\n",
      "0 -0.375756  0.103879  0.056566  ...  0.034410 -0.226018 -0.120518 -0.226665   \n",
      "1  0.411562  0.095954 -0.269742  ...  1.205944 -0.261137 -0.323417 -0.265730   \n",
      "2  0.182742  0.210657 -0.247041  ... -0.263745 -0.199823 -0.633805 -0.188395   \n",
      "3  0.214786  0.535203 -0.089594  ... -0.278290 -0.221613 -0.691776 -0.232808   \n",
      "4  0.558822  0.436541  0.179609  ... -0.270290 -0.227409 -0.496123 -0.222385   \n",
      "\n",
      "        576       577        585       589  590_1                 Date  \n",
      "0 -0.229797 -0.135520  -0.197057 -0.295753      0  19/07/2008 11:55:00  \n",
      "1 -0.263493 -0.460054   0.385113  1.156846      0  19/07/2008 12:32:00  \n",
      "2 -0.205046 -0.590505   0.029888 -0.178955      1  19/07/2008 13:17:00  \n",
      "3 -0.224950 -0.645708  -0.283360 -0.275049      0  19/07/2008 14:43:00  \n",
      "4 -0.230791 -0.454486  26.913337 -0.275049      0  19/07/2008 15:22:00  \n",
      "\n",
      "[5 rows x 299 columns]\n"
     ]
    }
   ],
   "source": [
    "# Scale dataset \n",
    "secom_inputs=secom_df.loc[:,'0':'589']\n",
    "target=secom_df.loc[:,'590_1']\n",
    "Date=secom_df.loc[:,'591']\n",
    "inputs_scaled=pd.DataFrame(preprocessing.scale(secom_inputs),columns=secom_inputs.columns)\n",
    "secom_scaled=inputs_scaled\n",
    "secom_scaled.loc[:,'590_1']=target\n",
    "secom_scaled.loc[:,'Date']=Date\n",
    "secom=secom_scaled.copy()\n",
    "print(secom.shape)\n",
    "print(secom.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>6</th>\n",
       "      <th>12</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>...</th>\n",
       "      <th>570</th>\n",
       "      <th>571</th>\n",
       "      <th>572</th>\n",
       "      <th>573</th>\n",
       "      <th>574</th>\n",
       "      <th>576</th>\n",
       "      <th>577</th>\n",
       "      <th>585</th>\n",
       "      <th>589</th>\n",
       "      <th>590_1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2008-07-19 11:55:00</td>\n",
       "      <td>0.224463</td>\n",
       "      <td>0.849523</td>\n",
       "      <td>-0.436430</td>\n",
       "      <td>0.035804</td>\n",
       "      <td>-0.050121</td>\n",
       "      <td>-0.564354</td>\n",
       "      <td>0.763117</td>\n",
       "      <td>-0.375756</td>\n",
       "      <td>0.103879</td>\n",
       "      <td>0.056566</td>\n",
       "      <td>...</td>\n",
       "      <td>0.190142</td>\n",
       "      <td>0.034410</td>\n",
       "      <td>-0.226018</td>\n",
       "      <td>-0.120518</td>\n",
       "      <td>-0.226665</td>\n",
       "      <td>-0.229797</td>\n",
       "      <td>-0.135520</td>\n",
       "      <td>-0.197057</td>\n",
       "      <td>-0.295753</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2008-07-19 12:32:00</td>\n",
       "      <td>1.107287</td>\n",
       "      <td>-0.383106</td>\n",
       "      <td>1.016977</td>\n",
       "      <td>0.155282</td>\n",
       "      <td>-0.059585</td>\n",
       "      <td>0.197639</td>\n",
       "      <td>0.181528</td>\n",
       "      <td>0.411562</td>\n",
       "      <td>0.095954</td>\n",
       "      <td>-0.269742</td>\n",
       "      <td>...</td>\n",
       "      <td>0.256816</td>\n",
       "      <td>1.205944</td>\n",
       "      <td>-0.261137</td>\n",
       "      <td>-0.323417</td>\n",
       "      <td>-0.265730</td>\n",
       "      <td>-0.263493</td>\n",
       "      <td>-0.460054</td>\n",
       "      <td>0.385113</td>\n",
       "      <td>1.156846</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2008-07-19 13:17:00</td>\n",
       "      <td>-1.114000</td>\n",
       "      <td>0.798901</td>\n",
       "      <td>-0.481447</td>\n",
       "      <td>0.688278</td>\n",
       "      <td>-0.047447</td>\n",
       "      <td>-0.906768</td>\n",
       "      <td>0.633530</td>\n",
       "      <td>0.182742</td>\n",
       "      <td>0.210657</td>\n",
       "      <td>-0.247041</td>\n",
       "      <td>...</td>\n",
       "      <td>0.257279</td>\n",
       "      <td>-0.263745</td>\n",
       "      <td>-0.199823</td>\n",
       "      <td>-0.633805</td>\n",
       "      <td>-0.188395</td>\n",
       "      <td>-0.205046</td>\n",
       "      <td>-0.590505</td>\n",
       "      <td>0.029888</td>\n",
       "      <td>-0.178955</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2008-07-19 14:43:00</td>\n",
       "      <td>-0.350156</td>\n",
       "      <td>-0.199072</td>\n",
       "      <td>-0.051705</td>\n",
       "      <td>-1.104376</td>\n",
       "      <td>-0.050831</td>\n",
       "      <td>0.502662</td>\n",
       "      <td>0.581382</td>\n",
       "      <td>0.214786</td>\n",
       "      <td>0.535203</td>\n",
       "      <td>-0.089594</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002548</td>\n",
       "      <td>-0.278290</td>\n",
       "      <td>-0.221613</td>\n",
       "      <td>-0.691776</td>\n",
       "      <td>-0.232808</td>\n",
       "      <td>-0.224950</td>\n",
       "      <td>-0.645708</td>\n",
       "      <td>-0.283360</td>\n",
       "      <td>-0.275049</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2008-07-19 15:22:00</td>\n",
       "      <td>0.242296</td>\n",
       "      <td>0.087328</td>\n",
       "      <td>1.117227</td>\n",
       "      <td>-0.156616</td>\n",
       "      <td>-0.047033</td>\n",
       "      <td>-0.115954</td>\n",
       "      <td>0.610329</td>\n",
       "      <td>0.558822</td>\n",
       "      <td>0.436541</td>\n",
       "      <td>0.179609</td>\n",
       "      <td>...</td>\n",
       "      <td>0.085279</td>\n",
       "      <td>-0.270290</td>\n",
       "      <td>-0.227409</td>\n",
       "      <td>-0.496123</td>\n",
       "      <td>-0.222385</td>\n",
       "      <td>-0.230791</td>\n",
       "      <td>-0.454486</td>\n",
       "      <td>26.913337</td>\n",
       "      <td>-0.275049</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 298 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            0         1         2         3         4  \\\n",
       "Date                                                                    \n",
       "2008-07-19 11:55:00  0.224463  0.849523 -0.436430  0.035804 -0.050121   \n",
       "2008-07-19 12:32:00  1.107287 -0.383106  1.016977  0.155282 -0.059585   \n",
       "2008-07-19 13:17:00 -1.114000  0.798901 -0.481447  0.688278 -0.047447   \n",
       "2008-07-19 14:43:00 -0.350156 -0.199072 -0.051705 -1.104376 -0.050831   \n",
       "2008-07-19 15:22:00  0.242296  0.087328  1.117227 -0.156616 -0.047033   \n",
       "\n",
       "                            6        12        14        15        16  ...  \\\n",
       "Date                                                                   ...   \n",
       "2008-07-19 11:55:00 -0.564354  0.763117 -0.375756  0.103879  0.056566  ...   \n",
       "2008-07-19 12:32:00  0.197639  0.181528  0.411562  0.095954 -0.269742  ...   \n",
       "2008-07-19 13:17:00 -0.906768  0.633530  0.182742  0.210657 -0.247041  ...   \n",
       "2008-07-19 14:43:00  0.502662  0.581382  0.214786  0.535203 -0.089594  ...   \n",
       "2008-07-19 15:22:00 -0.115954  0.610329  0.558822  0.436541  0.179609  ...   \n",
       "\n",
       "                          570       571       572       573       574  \\\n",
       "Date                                                                    \n",
       "2008-07-19 11:55:00  0.190142  0.034410 -0.226018 -0.120518 -0.226665   \n",
       "2008-07-19 12:32:00  0.256816  1.205944 -0.261137 -0.323417 -0.265730   \n",
       "2008-07-19 13:17:00  0.257279 -0.263745 -0.199823 -0.633805 -0.188395   \n",
       "2008-07-19 14:43:00  0.002548 -0.278290 -0.221613 -0.691776 -0.232808   \n",
       "2008-07-19 15:22:00  0.085279 -0.270290 -0.227409 -0.496123 -0.222385   \n",
       "\n",
       "                          576       577        585       589  590_1  \n",
       "Date                                                                 \n",
       "2008-07-19 11:55:00 -0.229797 -0.135520  -0.197057 -0.295753      0  \n",
       "2008-07-19 12:32:00 -0.263493 -0.460054   0.385113  1.156846      0  \n",
       "2008-07-19 13:17:00 -0.205046 -0.590505   0.029888 -0.178955      1  \n",
       "2008-07-19 14:43:00 -0.224950 -0.645708  -0.283360 -0.275049      0  \n",
       "2008-07-19 15:22:00 -0.230791 -0.454486  26.913337 -0.275049      0  \n",
       "\n",
       "[5 rows x 298 columns]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the date column as an index \n",
    "secom.index = pd.to_datetime(secom['Date'], format='%d/%m/%Y %H:%M:%S')\n",
    "secom.drop(['Date'],axis=1,inplace=True)\n",
    "secom.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Section two include the following steps:  \n",
    "- Use the features that were already selected in milestone 1  \n",
    "- Create SMOT object to take care of the imbalanced data\n",
    "- Split the dataset into train-test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features selected from milestone 1\n",
    "inputs= secom[['21','28','32','33','59','64','122','123','124','125','127','129','195','197','198','430','431','467','510','519']]\n",
    "target= secom[['590_1']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 1], dtype=uint8), array([1463,  104], dtype=int64))\n",
      "(array([0, 1], dtype=uint8), array([1463, 1463], dtype=int64))\n",
      "X_SMOT shape (2926, 20)\n",
      "Y_SMOT shape (2926,)\n"
     ]
    }
   ],
   "source": [
    "# create a SMOTE object\n",
    "sm = SMOTE(random_state=42)\n",
    "\n",
    "# use SMOTE to fit the data in X and y\n",
    "X_SMOT, Y_SMOT = sm.fit_sample(inputs, target.values.ravel())\n",
    "print(np.unique(target, return_counts=True))\n",
    "print(np.unique(Y_SMOT, return_counts=True))\n",
    "print ('X_SMOT shape',X_SMOT.shape)\n",
    "print ('Y_SMOT shape',Y_SMOT.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(878, 20)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Split data after SMOT into training and testing\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_SMOT, Y_SMOT, test_size=0.30, random_state=0)\n",
    "X_train.shape\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section three include the following:  \n",
    "### 1 - Build a simple neural network model   \n",
    "### 2- Build a DNN model  \n",
    "### 3- Build a RNN model  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 - Build a simple neural network model  \n",
    "- The first layer is the input layer  \n",
    "- The next layer is  with 100 units and sofmax activation function \n",
    "- The output layer must create 1 output value \n",
    "- Activation function is relu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2048 samples\n",
      "Epoch 1/20\n",
      "2048/2048 [==============================] - 0s 170us/sample - loss: 0.6222 - accuracy: 0.7026\n",
      "Epoch 2/20\n",
      "2048/2048 [==============================] - 0s 92us/sample - loss: 0.6881 - accuracy: 0.7788\n",
      "Epoch 3/20\n",
      "2048/2048 [==============================] - 0s 91us/sample - loss: 0.8793 - accuracy: 0.7192\n",
      "Epoch 4/20\n",
      "2048/2048 [==============================] - 0s 91us/sample - loss: 2.4250 - accuracy: 0.7334\n",
      "Epoch 5/20\n",
      "2048/2048 [==============================] - 0s 91us/sample - loss: 3.0464 - accuracy: 0.6729\n",
      "Epoch 6/20\n",
      "2048/2048 [==============================] - 0s 98us/sample - loss: 2.7170 - accuracy: 0.6953\n",
      "Epoch 7/20\n",
      "2048/2048 [==============================] - 0s 96us/sample - loss: 2.4961 - accuracy: 0.6895\n",
      "Epoch 8/20\n",
      "2048/2048 [==============================] - 0s 100us/sample - loss: 1.9384 - accuracy: 0.6807\n",
      "Epoch 9/20\n",
      "2048/2048 [==============================] - 0s 113us/sample - loss: 1.7630 - accuracy: 0.7026\n",
      "Epoch 10/20\n",
      "2048/2048 [==============================] - 0s 96us/sample - loss: 1.7323 - accuracy: 0.7100\n",
      "Epoch 11/20\n",
      "2048/2048 [==============================] - 0s 90us/sample - loss: 1.7128 - accuracy: 0.7124s - loss: 1.6269 - accuracy: 0.\n",
      "Epoch 12/20\n",
      "2048/2048 [==============================] - 0s 92us/sample - loss: 1.6933 - accuracy: 0.7163\n",
      "Epoch 13/20\n",
      "2048/2048 [==============================] - 0s 92us/sample - loss: 1.7165 - accuracy: 0.7173\n",
      "Epoch 14/20\n",
      "2048/2048 [==============================] - 0s 91us/sample - loss: 1.6535 - accuracy: 0.7202\n",
      "Epoch 15/20\n",
      "2048/2048 [==============================] - 0s 102us/sample - loss: 1.6970 - accuracy: 0.7124\n",
      "Epoch 16/20\n",
      "2048/2048 [==============================] - 0s 109us/sample - loss: 1.8127 - accuracy: 0.7119\n",
      "Epoch 17/20\n",
      "2048/2048 [==============================] - 0s 111us/sample - loss: 1.7015 - accuracy: 0.7236\n",
      "Epoch 18/20\n",
      "2048/2048 [==============================] - 0s 105us/sample - loss: 1.6362 - accuracy: 0.7305\n",
      "Epoch 19/20\n",
      "2048/2048 [==============================] - 0s 95us/sample - loss: 1.5282 - accuracy: 0.7402\n",
      "Epoch 20/20\n",
      "2048/2048 [==============================] - 0s 91us/sample - loss: 1.8848 - accuracy: 0.7271\n",
      "\n",
      "\n",
      "Test loss, accuracy: [1.446031607297124, 0.73690206]\n"
     ]
    }
   ],
   "source": [
    "# Build a simple neural network model\n",
    "tf.compat.v1.random.set_random_seed(7299)\n",
    "\n",
    "tfclf = keras.Sequential()\n",
    "tfclf.add(layers.Flatten())\n",
    "# Let's add ONE hidden layers of 100 units each\n",
    "\n",
    "tfclf.add(layers.Dense(100,activation='softmax',input_dim=20))\n",
    "\n",
    "\n",
    "# also have to add the output layer\n",
    "tfclf.add(layers.Dense(1,activation='relu'))\n",
    "\n",
    "# pick an optimizer\n",
    "# for the complete list of optimizers see https://keras.io/optimizers/\n",
    "optimizer = keras.optimizers.Adam(learning_rate = 0.05)\n",
    "\n",
    "# compile the model\n",
    "# the metrics parameter is just for reporting\n",
    "tfclf.compile(optimizer = optimizer,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Fit and predict\n",
    "tfclf.fit(X_train, Y_train, epochs=20, batch_size=10, verbose=1)\n",
    "\n",
    "# Results\n",
    "result = tfclf.evaluate(X_test, Y_test, verbose=0)\n",
    "print('\\n\\nTest loss, accuracy:', result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.59      0.70       458\n",
      "           1       0.67      0.90      0.77       420\n",
      "\n",
      "    accuracy                           0.74       878\n",
      "   macro avg       0.77      0.74      0.73       878\n",
      "weighted avg       0.77      0.74      0.73       878\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ANN classification report\n",
    "pred_Y = tfclf.predict_classes(X_test, batch_size=1)\n",
    "print(classification_report(Y_test, pred_Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2- Build a DNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2048 samples\n",
      "Epoch 1/20\n",
      "2048/2048 [==============================] - 0s 226us/sample - loss: 0.5797 - accuracy: 0.7075\n",
      "Epoch 2/20\n",
      "2048/2048 [==============================] - 0s 102us/sample - loss: 0.5130 - accuracy: 0.7568\n",
      "Epoch 3/20\n",
      "2048/2048 [==============================] - 0s 100us/sample - loss: 0.4792 - accuracy: 0.7725\n",
      "Epoch 4/20\n",
      "2048/2048 [==============================] - 0s 99us/sample - loss: 0.4409 - accuracy: 0.7930\n",
      "Epoch 5/20\n",
      "2048/2048 [==============================] - 0s 100us/sample - loss: 0.4278 - accuracy: 0.8066\n",
      "Epoch 6/20\n",
      "2048/2048 [==============================] - 0s 130us/sample - loss: 0.4254 - accuracy: 0.8086\n",
      "Epoch 7/20\n",
      "2048/2048 [==============================] - 0s 135us/sample - loss: 0.3957 - accuracy: 0.8257\n",
      "Epoch 8/20\n",
      "2048/2048 [==============================] - 0s 113us/sample - loss: 0.4092 - accuracy: 0.8193\n",
      "Epoch 9/20\n",
      "2048/2048 [==============================] - 0s 100us/sample - loss: 0.3883 - accuracy: 0.8330\n",
      "Epoch 10/20\n",
      "2048/2048 [==============================] - 0s 100us/sample - loss: 0.3844 - accuracy: 0.8374\n",
      "Epoch 11/20\n",
      "2048/2048 [==============================] - 0s 103us/sample - loss: 0.3621 - accuracy: 0.8481\n",
      "Epoch 12/20\n",
      "2048/2048 [==============================] - 0s 105us/sample - loss: 0.3669 - accuracy: 0.8403\n",
      "Epoch 13/20\n",
      "2048/2048 [==============================] - 0s 102us/sample - loss: 0.3533 - accuracy: 0.8560\n",
      "Epoch 14/20\n",
      "2048/2048 [==============================] - 0s 100us/sample - loss: 0.3429 - accuracy: 0.8574\n",
      "Epoch 15/20\n",
      "2048/2048 [==============================] - 0s 120us/sample - loss: 0.3401 - accuracy: 0.8555\n",
      "Epoch 16/20\n",
      "2048/2048 [==============================] - 0s 131us/sample - loss: 0.3301 - accuracy: 0.8579\n",
      "Epoch 17/20\n",
      "2048/2048 [==============================] - 0s 146us/sample - loss: 0.3211 - accuracy: 0.8701\n",
      "Epoch 18/20\n",
      "2048/2048 [==============================] - 0s 139us/sample - loss: 0.3256 - accuracy: 0.8599\n",
      "Epoch 19/20\n",
      "2048/2048 [==============================] - 0s 123us/sample - loss: 0.3262 - accuracy: 0.8579\n",
      "Epoch 20/20\n",
      "2048/2048 [==============================] - 0s 108us/sample - loss: 0.3255 - accuracy: 0.8618\n",
      "\n",
      "\n",
      "Test loss, accuracy: [0.28511810703266727, 0.8883827]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tfclf2 =  tf.keras.Sequential()\n",
    "\n",
    "# The first layer flattens the input\n",
    "tfclf2.add(layers.Flatten())\n",
    "tfclf2.add(layers.Dense(25,activation='sigmoid',input_shape=(20,)))\n",
    "tfclf2.add(layers.Dropout(0.2))\n",
    "tfclf2.add(layers.Dense(25, activation='sigmoid'))\n",
    "tfclf2.add(layers.Dropout(0.3))\n",
    "\n",
    "\n",
    "\n",
    "# There are 1 output categories, so we need ten \n",
    "tfclf2.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate = 0.05)\n",
    "\n",
    "# compile the model\n",
    "\n",
    "tfclf2.compile(optimizer = optimizer,\n",
    "              loss='binary_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Fit and predict\n",
    "history = tfclf2.fit(X_train, Y_train,epochs=20, batch_size=10, verbose=1)\n",
    "Yptest = tfclf2.evaluate(X_test, Y_test, verbose=0)\n",
    "print('\\n\\nTest loss, accuracy:', Yptest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.81      0.88       458\n",
      "           1       0.82      0.98      0.89       420\n",
      "\n",
      "    accuracy                           0.89       878\n",
      "   macro avg       0.90      0.89      0.89       878\n",
      "weighted avg       0.90      0.89      0.89       878\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DNN classification report\n",
    "pred_y = tfclf2.predict_classes(X_test, batch_size=1)\n",
    "print(classification_report(Y_test, pred_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  3-Build a RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape the data so that each time point becomes a \"batch\"\n",
    "XX_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "XX_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an RNN network\n",
    "rnn = keras.Sequential()\n",
    "\n",
    "# add  RNN layers\n",
    "rnn.add(layers.SimpleRNN(10, input_shape=(1, 20), stateful=True, batch_size=1,return_sequences=True))\n",
    "rnn.add(layers.SimpleRNN(10,  stateful=True, batch_size=1,return_sequences=True))\n",
    "rnn.add(layers.SimpleRNN(10,  stateful=True, batch_size=1,return_sequences=True))\n",
    "rnn.add(layers.SimpleRNN(10,  stateful=True, batch_size=1))\n",
    "# The output layer is an ordinary Dense layer with one (sigmoid) output\n",
    "rnn.add(layers.Dense(1, activation='sigmoid'))  \n",
    "\n",
    "# compile the model as usual\n",
    "# We'll use MSE as the loss function \n",
    "# (binary cross entropy does not work so well here)\n",
    "rnn.compile(loss='mse', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_61\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_19 (SimpleRNN)    (1, 1, 10)                310       \n",
      "_________________________________________________________________\n",
      "simple_rnn_20 (SimpleRNN)    (1, 1, 10)                210       \n",
      "_________________________________________________________________\n",
      "simple_rnn_21 (SimpleRNN)    (1, 1, 10)                210       \n",
      "_________________________________________________________________\n",
      "simple_rnn_22 (SimpleRNN)    (1, 10)                   210       \n",
      "_________________________________________________________________\n",
      "dense_182 (Dense)            (1, 1)                    11        \n",
      "=================================================================\n",
      "Total params: 951\n",
      "Trainable params: 951\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# we can print out a summary of our model\n",
    "rnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2048 samples\n",
      "Epoch 1/20\n",
      "2048/2048 [==============================] - 11s 6ms/sample - loss: 0.2146 - accuracy: 0.6631\n",
      "Epoch 2/20\n",
      "2048/2048 [==============================] - 11s 5ms/sample - loss: 0.1833 - accuracy: 0.7334\n",
      "Epoch 3/20\n",
      "2048/2048 [==============================] - 11s 5ms/sample - loss: 0.1761 - accuracy: 0.7451\n",
      "Epoch 4/20\n",
      "2048/2048 [==============================] - 11s 5ms/sample - loss: 0.1707 - accuracy: 0.7568\n",
      "Epoch 5/20\n",
      "2048/2048 [==============================] - 11s 5ms/sample - loss: 0.1654 - accuracy: 0.7729\n",
      "Epoch 6/20\n",
      "2048/2048 [==============================] - 11s 5ms/sample - loss: 0.1606 - accuracy: 0.7783\n",
      "Epoch 7/20\n",
      "2048/2048 [==============================] - 11s 5ms/sample - loss: 0.1559 - accuracy: 0.7886\n",
      "Epoch 8/20\n",
      "2048/2048 [==============================] - 11s 5ms/sample - loss: 0.1513 - accuracy: 0.7969\n",
      "Epoch 9/20\n",
      "2048/2048 [==============================] - 11s 5ms/sample - loss: 0.1463 - accuracy: 0.8042\n",
      "Epoch 10/20\n",
      "2048/2048 [==============================] - 11s 5ms/sample - loss: 0.1411 - accuracy: 0.8164\n",
      "Epoch 11/20\n",
      "2048/2048 [==============================] - 11s 5ms/sample - loss: 0.1356 - accuracy: 0.8198\n",
      "Epoch 12/20\n",
      "2048/2048 [==============================] - 11s 5ms/sample - loss: 0.1294 - accuracy: 0.8306\n",
      "Epoch 13/20\n",
      "2048/2048 [==============================] - 11s 5ms/sample - loss: 0.1237 - accuracy: 0.8364\n",
      "Epoch 14/20\n",
      "2048/2048 [==============================] - 11s 5ms/sample - loss: 0.1194 - accuracy: 0.8418\n",
      "Epoch 15/20\n",
      "2048/2048 [==============================] - 11s 5ms/sample - loss: 0.1145 - accuracy: 0.8530\n",
      "Epoch 16/20\n",
      "2048/2048 [==============================] - 11s 5ms/sample - loss: 0.1087 - accuracy: 0.8667\n",
      "Epoch 17/20\n",
      "2048/2048 [==============================] - 11s 5ms/sample - loss: 0.1063 - accuracy: 0.8726\n",
      "Epoch 18/20\n",
      "2048/2048 [==============================] - 11s 5ms/sample - loss: 0.1027 - accuracy: 0.8750\n",
      "Epoch 19/20\n",
      "2048/2048 [==============================] - 11s 5ms/sample - loss: 0.0993 - accuracy: 0.8809\n",
      "Epoch 20/20\n",
      "2048/2048 [==============================] - 11s 5ms/sample - loss: 0.0960 - accuracy: 0.8853\n",
      "\n",
      "\n",
      "Test loss, accuracy: [0.1562798189257252, 0.79498863]\n"
     ]
    }
   ],
   "source": [
    "rnn.fit(XX_train, Y_train, epochs=20, batch_size=1, verbose=1, shuffle=False)# no shuffle casue it is a timeseries\n",
    "Yptest3 = rnn.evaluate(XX_test, Y_test, verbose=0)\n",
    "print('\\n\\nTest loss, accuracy:', Yptest3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.70      0.78       458\n",
      "           1       0.73      0.90      0.81       420\n",
      "\n",
      "    accuracy                           0.80       878\n",
      "   macro avg       0.81      0.80      0.80       878\n",
      "weighted avg       0.81      0.80      0.79       878\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RNN classification report\n",
    "XX_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "pred_y3= rnn.predict_classes(XX_test, batch_size=1)\n",
    "print(classification_report(Y_test, pred_y3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary:\n",
    "To prepare our dataset for analysis I have applied several data cleaning techniques and reduced the feature count from 590 to 299. I also applied SMOT to balance the target variable. I included only 20 features in the various neural network models. \n",
    "\n",
    "The simple neural network achieves accuracy of ~74% with recall for class 0 (faulty) is 0.59.  \n",
    "The deep neural network achieves accuracy ~87% with an imrovement of recall to 0.81.   \n",
    "The recurrent neural network achieves accuracy ~0.80 and recall 0.73.  \n",
    "\n",
    "Looking at the past milestones, the accuracy was around ~77% for the decision tree model, ~83% for the ensemble model and ~89% for SVM model.  \n",
    "\n",
    "It is well known that RNN are for sequences. If data is sequential in nature (time series) than it is preferable to use RNN over DNN and other \"static\" models.  \n",
    "\n",
    "We can observe from my notebook that DNN performed the best, however such sequential datasets are much better addressed by RNN than non-recurrent models. For that, my recommendation for the Secom analysis is to use LSTM-based models and look out for the vanishing gradient problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
